{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ab19ed",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144ad08f",
   "metadata": {},
   "source": [
    "**BERT**, which stands for *Bidirectional Encoder Representations from Transformers*, is a method of pre-training language representations that was introduced by researchers at Google AI Language in 2018\n",
    "\n",
    "<img src=\"https://www.aionlinecourse.com/uploads/blog/image/BERT1.png\" width=\"600\" height=\"200\" />\n",
    "\n",
    "It's designed to understand the context and relations among words in a sentence. Unlike traditional word embeddings like Word2Vec, which generate a single word embedding for each word, BERT examines the context of each occurrence of a word in a bidirectional manner—meaning it looks at the words before and after the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8f663",
   "metadata": {},
   "source": [
    "## Differences from Classical Transformer:\n",
    "\n",
    "The classical Transformer model, introduced in the paper \"Attention is All You Need\" by Vaswani et al. (2017), consists of an encoder and a decoder. The encoder reads the input text, and the decoder produces the output text. This type of architecture excels at sequence-to-sequence tasks like machine translation.\n",
    "\n",
    "Here are the key differences between BERT and the classical Transformer:\n",
    "\n",
    "- **Directionality**:\n",
    "    - BERT: Bidirectional context is used to understand the relationship between words in a sentence by analyzing the words that come before and after a given word.\n",
    "    - Transformer: Processes all words in parallel and doesn’t inherently have a sense of word order (i.e., it treats the input words in order or reversely the same unless positional encodings are added).\n",
    "\n",
    "- **Model Architecture**:\n",
    "     - BERT: Utilizes only the encoder part of the Transformer architecture.\n",
    "     - Transformer: Comprises both an encoder and a decoder.\n",
    "\n",
    "- **Training Objective**:\n",
    "     - BERT: Is trained on a masked language modeling task and next sentence prediction.\n",
    "     - Transformer: Is trained to map input sequences to output sequences, like in machine translation.\n",
    "\n",
    "- **Usage**:\n",
    "     - BERT: More suited for tasks that require understanding of context and relationships between parts of a sentence.\n",
    "     - Transformer: Better suited for sequence-to-sequence tasks.\n",
    "\n",
    "- **Data**:\n",
    "     - BERT: Can be fine-tuned on a smaller dataset after pre-training.\n",
    "     - Transformer: Typically requires a larger amount of training data and might not benefit from pre-training as much as BERT does.\n",
    "\n",
    "BERT's bidirectional understanding and the utilization of pre-training followed by fine-tuning have propelled its performance to state-of-the-art levels across a broad spectrum of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b941774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "#https://huggingface.co/docs/transformers/index\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b85e9d",
   "metadata": {},
   "source": [
    "Other models that you can fine-tune    \n",
    "- RoBERTa:\n",
    "A robustly optimized BERT approach that modifies key hyperparameters in BERT, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates.\n",
    "\n",
    "- GPT-2 and its variants like GPT-Large, GPT-Medium, and GPT-XL:\n",
    " GPT-2 is well-suited for tasks like text generation, and it can also be fine-tuned for various downstream tasks\n",
    "\n",
    "- DistilBERT:\n",
    "    A smaller, faster, cheaper, and lighter version of BERT which is trained by distilling BERT base. It can be fine-tuned for various tasks like its teacher model, BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a98e9d",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "506cecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with loading your actual data\n",
    "texts = ['I love programming!', 'Python is awesome', 'I hate bugs']\n",
    "labels = [1, 1, 0] #1 positive, 0 negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24515c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_input = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b44df076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a61069",
   "metadata": {},
   "source": [
    "A downstream task is a specific task for which a pre-trained model is fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907a6ec",
   "metadata": {},
   "source": [
    "The specific variant \"BERT-base-uncased\" denotes a particular configuration of BERT that has been trained with certain specifics:\n",
    "\n",
    "- **Uncased**:\n",
    "The term \"uncased\" signifies that the text data used for training was converted to lowercase before the training process commenced, hence the model does not differentiate between uppercase and lowercase letters.\n",
    "\n",
    "- **Base**:\n",
    "The term \"base\" refers to the size and architecture of the BERT model. Specifically, BERT-base is the smaller variant with 12 layers (transformer blocks), 12 attention heads, and 110 million parameters. This is in contrast to BERT-large which has 24 layers, 16 attention heads, and 340 million parameters.\n",
    "\n",
    "- **Training Data**:\n",
    "BERT-base-uncased was trained on two large-scale datasets:\n",
    "   - BooksCorpus: A dataset containing 800 million words from 11,038 unpublished books.\n",
    "   - English Wikipedia: The text from the English Wikipedia, amounting to 2,500 million words. However, only the text and not any lists, tables, or headers were included.\n",
    "\n",
    "- **Training Task**:\n",
    "BERT was pre-trained using two unsupervised tasks:\n",
    " - Masked Language Modeling (MLM): Randomly masking out words and training the model to predict them based on the surrounding context.\n",
    "  - Next Sentence Prediction (NSP): Training the model to predict whether two sentences come consecutively in a text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eeb9f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(tokenized_input['input_ids'], tokenized_input['attention_mask'], torch.tensor(labels))\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81bca8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f005116",
   "metadata": {},
   "source": [
    "In the context of transformer models such as BERT, tokenization involves converting each text into a sequence of tokens, which are then mapped to a sequence of numerical indices corresponding to a model's vocabulary. In addition to this, the model requires a couple of additional inputs to work correctly. One of these is the \"attention mask\", which is represented by the tokenized_input['attention_mask'] in your code.\n",
    "\n",
    "The attention mask is a binary mask that indicates which tokens are actual words and which are padding tokens, added to ensure that all sequences are the same length for batch processing. A value of 1 in the attention mask indicates a real token, while a value of 0 indicates a padding token.\n",
    "\n",
    "In the tensor above:\n",
    "```python out\n",
    "    The first row [1, 1, 1, 1, 1, 1] indicates that all the tokens in the first text sequence are real tokens, with no padding tokens.\n",
    "    The second and third rows [1, 1, 1, 1, 1, 0] indicate that the last token in these text sequences is a padding token, as denoted by the 0 value, while all preceding tokens are real tokens.\n",
    "```\n",
    "The attention mask is crucial for the transformer model to correctly apply self-attention mechanisms only to the real tokens in the sequence, and ignore the padding tokens. This way, the model can process batches of sequences efficiently while still producing meaningful outputs.\n",
    "\n",
    "*p.s.*\n",
    "    Not all sequences (text) that you have are of the same length, yet many machine learning models require input data to be fed in batches with consistent dimensions. This is where padding comes into play.\n",
    "A padding token is a special token that is added to the shorter sequences in a batch to make them all the same length. The padding token itself carries no real information and is typically ignored by the model during training and evaluation through the use of masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "35c9053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "#AdamW optimizer, which is a variant of the Adam optimizer with a different weight decay regularization scheme. \n",
    "#AdamW was proposed to fix the improper weight decay implementation in the original Adam optimizer, \n",
    "#leading to better generalization performance\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "#In PyTorch, Binary Cross Entropy loss is provided through the torch.nn.BCEWithLogitsLoss class. \n",
    "# This loss combines a Sigmoid layer and the BCELoss (Binary Cross Entropy Loss) in one single class, \n",
    "# which makes it more numerically stable. \n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f84ddc",
   "metadata": {},
   "source": [
    "[PyTorch AdamW Documentation](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ae9cb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad38e02504d45419f3bb322cedc4207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.775959312915802\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc56aacd85e4a5296956b1ab1bb53d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6330906748771667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7ace97d78a4555ad24965790d30ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.5627160668373108\n"
     ]
    }
   ],
   "source": [
    "model.train()  # Set the model to training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad() #to clear the old gradients, because by default, gradients in PyTorch accumulate\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader)}')\n",
    "    \n",
    "#p.s. The reason for zeroing the gradients at the beginning of each loop is to ensure that the gradients \n",
    "# computed in the current batch are solely due to the current batch and do not contain any remnant of the gradients \n",
    "# from previous batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d43247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I love data science\"  # Replace with your text\n",
    "tokens = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f78cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No gradient computation is needed\n",
    "    outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4fcf902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be3d7ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Positive\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"Negative\", 1: \"Positive\"}  # Adjust according to your task\n",
    "predicted_label = label_map[predicted_class]\n",
    "print(f'Predicted Label: {predicted_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe031e5d",
   "metadata": {},
   "source": [
    "# Multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d1dbb60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Assume you have a dataset with text and labels (2 for Positive, 1 for Neutral, 0 for Negative).\n",
    "texts = ['I love programming!', 'Python is awesome', 'I hate bugs', 'The code is okay']\n",
    "labels = [2, 2, 0, 1]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_input = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Specify the number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "25b34787",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(tokenized_input['input_ids'], tokenized_input['attention_mask'], torch.tensor(labels))\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d5d5c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d5cba0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0f32059ce244a4991d42aad405a07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1351358890533447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad38c8d46deb4beab9ab3507cd3fb422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.081408977508545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c008c23bff74a91bb2a2b4e2215197c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.0835492610931396\n"
     ]
    }
   ],
   "source": [
    "model.train()  # Set the model to training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a5439b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Positive\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Today I love apple\"  # Replace with your text\n",
    "tokens = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No gradient computation is needed\n",
    "    outputs = model(**tokens)\n",
    "\n",
    "\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2:\"Positive\"}  # Adjust according to your task\n",
    "predicted_label = label_map[predicted_class]\n",
    "print(f'Predicted Label: {predicted_label}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
